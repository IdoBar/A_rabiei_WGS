---
title: "Genome Sequencing of Ascochyta rabiei Strains"
author: "Ido Bar"
date: "18 July 2017"
output: 
    html_document:
      toc: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
bibliography: data/Fungal_genomes.bib
csl: data/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
cran_packages <- c("tidyverse", "knitr", "pander", "captioner", "bibtex", "RefManageR")
install.deps(cran_packages)
# Connect to Zotero to access references
biblio <- ReadBib("data/Fungal_genomes.bib")
```



```{r captions, include=FALSE, eval=TRUE}
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
tbls(name="samples","Ascochyta rabiei isolates used for DNA sequencing.")
samples_table <- readxl::read_excel("data/P_rabiei_isolate_list_sent_for_sequencing.xlsx")

#figs(name="WtFreq1","Weight frequency of Ruffe captured in 1992.")
```

# Experimental Design
DNA was extracted from 9 strains of _Ascochyta rabiei_ (`r tbls(name="samples",display="cite")`) and sent for sequencing on an Illumina HiSeq2500, producing 100 bp short paired-end reads (Macrogen, Korea; detailed sequencing report can be found at `Macrogen_report_1702KHP-0164.pdf` file).

```{r eval=TRUE} 
pander(as.data.frame(samples_table), caption=tbls("samples"), justify="left")
```


# Analysis Pipeline
## General overview:
1. Data pre-processing:
    a. Quality check
    b. Adaptor trimming
    c. Post-trim quality check
2. Mapping reads to a reference genome (keep unmapped)
3. Reads deduplication
4. Variant calling
5. _De novo_ genome assembly of unmapped reads
6. Identification of gene models 
7. Annotation of non-coding elements
8. Produce assembly statistics and assessment 

## Useful resources:

* Whole-Genome Comparison of _Aspergillus fumigatus_ Strains Serially Isolated from Patients with Aspergillosis. [@hagiwara_whole-genome_2014]:

> **Sequence analysis:** The Illumina data sets were trimmed using fastq-mcf in ea-utils (version 1.1.2-484), i.e., sequencing adapters and sequences with low quality scores (Phred score [Q], <30) were removed (24). The data sets were mapped to the genome sequence of the _A. fumigatus_ genome reference strain Af293 (29,420,142 bp, genome version s03-m04-r03) (25, 26) using Bowtie 2 (version 2.0.0-beta7) with the very sensitive option in end-to-end mode (27). Duplicated reads were removed using Picard (version 1.112) (<http://picard.sourceforge.net>). The programs mpileup and bcftools from SAMtools (version 0.1.19-44428cd) were used to perform further quality controls. In mpileup, the -q20 argument was used to trim reads with low-quality mapping, whereas the argument -q30 was used to trim low-quality bases at the 3' end (28). The bcftools setting was set to -c in order to call variants using Bayesian inference. Consensus and single nucleotide polymorphisms (SNPs) were excluded if they did not meet a minimum coverage of 5? or if the variant was present in <90% of the base calls (29, 30). The genotype field in the variant call format (VCF) files indicates homozygote and heterozygote probabilities as Phred-scaled likelihoods. SNPs were excluded if they were called as heterozygous genotypes using SAMtools. The mapping results were visualized in the Integrative Genomics Viewer (version 2.3.3) (31, 32). The reference genome data included information on open reading frames and annotations, from which the SNPs were designated nonsynonymous or synonymous.  
Single nucleotide mutations were confirmed by Sanger sequencing. Regions of approximately 400 bp that contained a mutation were amplified with appropriately designed primer pairs and then sequenced. The primer sequences are listed in Table S1 in the supplemental material, which were named as follows. For verification of the SNPs in strains from patient I or patient II, PaI or PaII was added to the primer name, respectively. For nonsynonymous SNPs, synonymous SNPs, or SNPs in a noncoding region, "(NS)," "(Syno)," or "(NonC)" was added to the primer name, respectively.  
**Analysis of unmapped reads:** _De novo_ assembly of the unmapped reads was conducted using the Newbler assembler 2.9 (Roche), with default parameters. The contigs were selected based on size/depth criteria: those of <500 bp and/or with a depth of <30? coverage were removed. To investigate whether unique genome sequences were present in strains isolated from the same patient, the unmapped reads of each strain were mapped to the contigs generated from all the strains in the same patient by the Bowtie 2 software. The coverage of the mapped regions was then evaluated. Gene predictions were performed using the gene prediction tool AUGUSTUS (version 2.5.5), with a training set of  _A. fumigatus_ (33). The parameters of AUGUSTUS were -species = aspergillus_fumigatus, -strand = both, -genemodel = partial, -singlestrand = false, -protein = on, -introns = on, -start = on, -stop = on, -cds = on, and -gff3 = on. To compare all the predicted genes with Aspergillus genes, consisting of 244,811 genes available on AspGD (34), a reciprocal BLAST best hit approach was performed by BLASTp (35), with an E value of 1.0e<sup>-4</sup>. All BLASTp results were filtered based on a BLASTp identity of $\ge80%$ and an aligned length coverage of $\ge80%$.

* Additional DNA-Seq data processing and genome assembly were performed following the methods specified by @haas_approaches_2011, @hittalmani_novo_2016 and @verma_draft_2016, with modification to run on the _Griffith Gowonda HPC Cluster_ (using PBSPro scheduler).  

## Methods
At the current pipeline, each step is processed for all files (horizontal processing) and the steps are called separately. However, since the pipeline is fairly simple and uses the same naming conventions (starting from a single `XXX.fq.gz` file), it might be more efficient to process it vertically, i.e., each file will be processed through all steps.
### Data pre-processing


#### Adaptor Trimming
Adaptors needed to be removed, as well as very low quality bases/reads, so trimming was performed with BBduk (from BBMap v37.36). See official download page on [SourceForge](https://sourceforge.net/projects/bbmap/), [user guide](http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/) and [SEQanswers thread](http://seqanswers.com/forums/showthread.php?t=42776).  
*Performed on 18/07/2017*
```{bash bbduk_trim}

# Prepare the BBduk commands
DATE=`date +%d_%m_%Y`
RUN=bbduk_grid_commands_${DATE} # day of run was 19_07_2017
mkdir Adapter_trim_${DATE}
cd !$
find ../ -maxdepth 1 -name "*.fastq.gz" | sort | gawk -F"\t" -v bbmap_dir=$BBMAP_DIR -v d=$DATE 'BEGIN{command=sprintf("bbduk.sh -Xmx1g ref=%s/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 hdist2=0 tpe tbo qtrim=w trimq=10 ziplevel=9 overwrite=true", bbmap_dir)};NR%2==1{n=split($1,a,"/"); infile=gensub("_1\\.", "_#.", "1", $1); basename=gensub(/(.+)_1.fastq.gz/, "\\1", "1", a[n]);printf("%s in=%s out=bbduk_clean_%s_R#.fastq.gz stats=%s.stats\n",command,infile,basename, basename)}' > ${RUN}.bash

CMDS_FILE=`ls -1 bbduk_grid_commands_*.bash`

# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -q qweek_s
#PBS -l select=1:ncpus=12:mem=4GB,walltime=10:00:00
#PBS -m be
#PBS -M i.bar@griffith.edu.au
#PBS -N A_rab_bbduk

cd $PBS_O_WORKDIR
CMDS_FILE=`ls -1 bbduk_grid_commands_*.bash`
gawk -v ARRAY_IND=$PBS_ARRAY_INDEX 'NR==ARRAY_IND' ${CMDS_FILE} | bash' > ${RUN}.pbs

# Run BBduk
JOBS_NUM=`wc -l ${CMDS_FILE} | gawk '{print $1}'`
qsub -J1-$JOBS_NUM ${RUN}.pbs
# record the array ID: 4722087

# Check results (23/01/2017)
find . -name "A_rab_bbduk.e4722087.*" -exec cat {} > ${RUN}.log \;
grep "Result:" ${RUN}.log | gawk 'BEGIN{per=0};match($7, /[0-9]+\.[0-9]+/){per+=substr($7, RSTART, RLENGTH); avg=per/NR}END{printf("Average percentage of bases kept after trimming: %.2f%% \nNumber of files processed: %d\n",avg, NR)}'
grep "Input:" ${RUN}.log | gawk 'BEGIN{reads=0};{reads+=$2; avg=reads/NR}END{printf("Average number of reads per file: %.2f \nTotal number of reads: %d\nNumber of files processed: %d\n",avg, reads,NR)}'
```
Output:  
```{bash mirnaseq_trim_results}
> "Average percentage of bases kept after trimming: 98.81%, Number of files processed: 9"  
> "Average number of reads per file: 36174722.22, Total number of reads: 325572500"  
```

**BBduk** seem to produce cleaner files (with less Illumina adapter and PCR primers contaminations) than other trimming methods, so it was chosen for downstream analysis.

Verify adaptor trimming quality with [DNApi v1.1](https://github.com/jnktsj/DNApi) [@tsuji_dnapi:_2016] and FastQC (v0.11.5)
```{bash trim_qc}
# Run FastQC on the output files
mkdir Fastqc_trimmed 
echo 'cd $PBS_O_WORKDIR; fastqc -t 12 -o Fastqc_trimmed bbduk_clean_*.fastq.gz' | qsub -V -q qweek_s -l select=1:ncpus=12:mem=4GB,walltime=10:00:00 -m be -M i.bar@griffith.edu.au -N fastq_trim
# Prepare DNApi commands
GENOME="$HOME/data/A_rabiei_sequencing/A.rabiei_me14"
bowtie2-build ${GENOME}.fasta $GENOME
DATE=`date +%d_%m_%Y`
ls -1 bbduk_clean_*.fastq.gz | gawk -v BTIDX=$GENOME -v date=$DATE 'BEGIN{logfile="DNApi_bbduk_"date".log"; "touch "logfile | getline};{printf "echo \"%s\" >> %s; pigz -cd %s | dnapi.py --no-output-files --map-command \"bowtie2 --very-sensitive -f -p 12 -x %s -U @in > @out \" - >> %s  \n", $1, logfile, $1, BTIDX, logfile}' > DNApi_bbduk_files_$DATE.bash

CMDS_FILE=`ls -1 DNApi_bbduk_files_*.bash`
# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -q qweek_s
#PBS -l select=1:ncpus=12:mem=4GB,walltime=10:00:00
#PBS -m be
#PBS -M i.bar@griffith.edu.au
#PBS -N DNApi

module load bioinformatics/samtools/0.1.18
module load glibc/2.14.1
cd $PBS_O_WORKDIR
CMDS_FILE=`ls -1 DNApi_bbduk_files_*.bash`
gawk -v ARRAY_IND=$PBS_ARRAY_INDEX "NR==ARRAY_IND" $CMDS_FILE | bash' > DNApi_bbduk_files_$DATE.pbs

# submit the jobs to the scheduler
PBS_FILE=`ls -1 DNApi_bbduk_files_*.pbs`
JOBS_NUM=`wc -l ${CMDS_FILE} | gawk '{print $1}'`
qsub -J1-${JOBS_NUM} $PBS_FILE
# Job ID 4722091[]
```

Get pre and post-trimming statistics using MultiQC v1.0 [@ewels_multiqc:_2016]
```{bash multi_qc}
# Install multiQC (make sure conda python environment is activated)
conda install -c bioconda multiqc
cd ~/data/A_rabiei_sequencing/Macrogen_sequences_1702KHP-0164/
multiqc .
```

### Mapping to reference genome
The trimmed reads were mapped to the _A. rabiei_ reference genome, strain Me14 [@verma_draft_2016] using Bowtie2 (version 2.3.2) with the `--very-sensitive` option in `--end-to-end` (global) mode. The resulting SAM files were processed to mark duplicates with SAMBLASTER v0.1.24 [@faust_samblaster:_2014] and converted into sorted BAM file with [Sambamba v0.6.6](http://lomereiter.github.io/sambamba/index.html).   
```{bash map_reads}
# Prepare sample meta data file
find $HOME/data/A_rabiei_sequencing/Macrogen_sequences_1702KHP-0164 -maxdepth 1 -name "*_?.fastq.gz" | gawk -vORS="\t" 'BEGIN{printf "strain\tread1_path\tread2_path\n"}NR%2==1{n=split($1,a,"/"); basename=gensub(/(.+)_1.fastq.gz/, "\\1", "1", a[n]); print basename,$1} NR%2==0{printf "%s\n", $1}' > ../sample.meta.tmp 
find $HOME/data/A_rabiei_sequencing/ -name "bbduk_clean_*.fastq.gz" | gawk -vORS="\t" 'BEGIN{printf "trimmed1_path\ttrimmed2_path\n"} NR%2==1{print $1} NR%2==0{printf "%s\n", $1}' | paste ../sample.meta.tmp - > ../sample.metadata

# install sambamba (need to fix internet connection to gowonda)
conda install -c bioconda sambamba
# Prepare bowtie2 commands
GENOME="$HOME/data/A_rabiei_sequencing/A.rabiei_me14"
mkdir BT2_map_me14 && cd !$
DATE=`date +%d_%m_%Y`
JOB_NAME="BT2_mapping_me14"
tail -n +2  $HOME/data/A_rabiei_sequencing/sample.metadata | gawk -v BTIDX=$GENOME -v date=$DATE '{printf "bowtie2 --very-sensitive -p 12 -x %s -1 %s -2 %s --un-gz %s_unmapped.fq.gz --no-unal | samblaster | sambamba view -S -f bam -t 12 /dev/stdin | sambamba sort -o %s_dedup.sorted.bam /dev/stdin \n", BTIDX, $4, $5, $1, $1}' > ${JOB_NAME}_${DATE}.cmds


# Prepare PBS script
echo '#!/bin/bash 
#PBS -V
#PBS -q qweek_s
#PBS -l select=1:ncpus=12:mem=4GB,walltime=10:00:00
#PBS -m be
#PBS -M i.bar@griffith.edu.au
#PBS -N BT2_me14

module load bioinformatics/bcftools/1.2
module load glibc/2.14.1
cd $PBS_O_WORKDIR
JOB_NAME="BT2_mapping_me14"
CMDS_FILE=`ls -1 ${JOB_NAME}*.cmds`
gawk -v ARRAY_IND=$PBS_ARRAY_INDEX "NR==ARRAY_IND" $CMDS_FILE | bash' > ${JOB_NAME}_${DATE}.pbs

# submit the jobs to the scheduler
PBS_FILE=`ls -1 ${JOB_NAME}_*.pbs`
CMDS_FILE=`ls -1 ${JOB_NAME}*.cmds`
JOBS_NUM=`wc -l ${CMDS_FILE} | gawk '{print $1}'`
qsub -J1-${JOBS_NUM} $PBS_FILE
# Job ID 4722091[]
```
*Alternatives:* Use other aligners such as STAR/BWA-mem for the mapping step and use the Picard command from the next step in the pipe to add Read Group, Sort and Index on the fly in one command (same as below, plus the option `SO=ccordinate`)

#### Add Read Group information
for each file (this step should have been done through the pipe after the mapping stage to reduce I/O):
```{bash add_RG}
DATE=`date +%d_%m_%Y`
JOB_NAME="Add_RGs"
ls -1 *_dedup.sorted.bam | gawk '{match($0, /(.+)_dedup.sorted.bam/, a); printf "picard AddOrReplaceReadGroups I=%s O=%s_dedup.rg.sorted.bam SM=%s ID=%s LB=%s_%s PL=Illumina PU=1702KHP CN=Macrogen_Korea PM=HiSeq2500 CREATE_INDEX=true \n", $1, a[1], a[1], NR, a[1], NR}' > ${JOB_NAME}_${DATE}.cmds 
# Prepare PBS script
echo '#!/bin/bash 
#PBS -V
#PBS -v JOB_NAME="Add_RGs"
#PBS -q qweek_s
#PBS -l select=1:ncpus=12:mem=4GB,walltime=10:00:00
#PBS -m be
#PBS -M i.bar@griffith.edu.au
#PBS -N add_rgs

cd $PBS_O_WORKDIR

CMDS_FILE=`ls -1 ${JOB_NAME}*.cmds`
gawk -v ARRAY_IND=$PBS_ARRAY_INDEX "NR==ARRAY_IND" $CMDS_FILE | bash' > ${JOB_NAME}_${DATE}.pbs
# submit the jobs to the scheduler 
CMDS_FILE=`ls -1 ${JOB_NAME}*.cmds`
JOBS_NUM=`wc -l ${CMDS_FILE} | gawk '{print $1}'`
qsub -J1-${JOBS_NUM} ${JOB_NAME}_${DATE}.pbs
# 4728358[].pbsserver
```

### Variant Calling and Filtering
The resulting BAM files (one per strain) were fed into `sambamba-mpileup` to assign variant probability score, which were then called by `bcftools call` (v1.2)
```{bash variant_call}
DATE=`date +%d_%m_%Y`
JOB_NAME="Var_calling"
# Prepare PBS script
echo '#!/bin/bash 
#PBS -V
#PBS -q qweek_s
#PBS -l select=1:ncpus=12:mem=4GB,walltime=10:00:00
#PBS -m be
#PBS -M i.bar@griffith.edu.au
#PBS -N var_call

module load bcftools/1.2
module load samtools/1.2
module load glibc/2.14.1
cd $PBS_O_WORKDIR
GENOME="$HOME/data/A_rabiei_sequencing/A.rabiei_me14"
samtools faidx $GENOME.fasta
BAM_FILES=`ls -1 *_dedup.rg.sorted.bam | gawk -vORS=" " "1"`
sambamba mpileup -t 12 $BAM_FILES --samtools -uf $GENOME.fasta | bcftools call -mv - > A_rabiei_BT2_on_Me14.vcf' > ${JOB_NAME}_${DATE}.pbs
# submit the jobs to the scheduler 
qsub ${JOB_NAME}_${DATE}.pbs
# 4728479.pbsserver
# Produce vcf stats
SAMPLES=`tail -n +2 $HOME/data/A_rabiei_sequencing/sample.metadata | gawk -vORS="," '{print $1}' | sed 's/,$//'`
bcftools stats -F $GENOME.fasta -s $SAMPLES A_rabiei_BT2_on_Me14.vcf > A_rabiei_BT2_on_Me14_vcf.stats
mkdir plots
plot-vcfstats -p plots/ A_rabiei_BT2_on_Me14_vcf.stats
```
Repeat the same with the `bcftools -c` flag to call using Bayesian consensus caller (`Var_calling_bayesian_24_07_2017.pbs` file). Then produce stats and plots as well:
```{bash vcf_stats}
# After manual editing of Var_calling_bayesian_24_07_2017.pbs file
qsub Var_calling_bayesian_24_07_2017.pbs
bcftools stats -F $GENOME.fasta -s $SAMPLES A_rabiei_BT2_mpileup_on_Me14_bayesian.vcf > A_rabiei_BT2_mpileup_on_Me14_bayesian.vcf.stats
mkdir mpileup_bcftools_consensus_plots
plot-vcfstats -p mpileup_bcftools_consensus_plots/ A_rabiei_BT2_mpileup_on_Me14_bayesian.vcf.stats
```


### General information
This document was last updated at `r Sys.time()` using R Markdown (built with `r R.version.string`). Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com> and [Rmarkdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).

***
### Bibliography

<!-- ```{r results='asis', eval=TRUE} -->
<!-- PrintBibliography(biblio) -->
<!-- ``` -->

