---
title: "Whole Genome Sequencing of <i>Ascochyta rabiei</i> Isolates"
author: "Ido Bar"
date: "18 July 2017"
always_allow_html: yes
output: 
    # md_document:
#      css: "style/style.css"
      # toc: true
      # toc_depth: 3
#      highlight: pygments
#      number_sections: false
    html_document:
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/Fungal_genomes.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# options(width = 180)
cran_packages <- c("tidyverse", "knitr", "pander", "captioner", "DT", "circlize", "htmltab", "vcfR", "R.utils")
pacman::p_load(char=cran_packages, repos="https://cran.rstudio.com/")
# load custom functions from github
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
# Connect to Zotero to access references
# biblio <- ReadBib("data/Fungal_genomes.bib") # , "bibtex", "RefManageR"
# Font Format
custom_font="consolas"
fontFmt = function(x,font="consolas"){
  #outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  #if (outputFormat == 'html')
  formatted_text <- sprintf("<font face='%s'>%s</font>",font,x)
  return(formatted_text)
  #else
  #  x
}
```



```{r captions, include=FALSE, eval=TRUE}
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
tbls(name="samples","Ascochyta rabiei isolates used for DNA sequencing.")
tbls(name="tfam","PLINK's .tfam file format.")
tbls(name="mapping_rates", "Mapping rates of the WGS reads to the Ascochyta rabiei Me14 reference genome.")
tbls(name="error_rates", "Comparison of the allelic error rates deriving from BBmap and Bowtie2 alignments (called with FreeBayes).")
tbls(name="mapping_sum", "Mapping statistics for 2017 and 2018 sequencing batches.")
figs(name="GC_cont", "GC (%) content in trimmed WGS reads.")
isolate_table <- readxl::read_excel("./sample_info/A_rabiei_isolate_list_for_wgs.xlsx", sheet = "Sequenced")
sequencing_table <- readxl::read_excel("./sample_info/A_rabiei_isolate_list_for_wgs.xlsx", sheet = "submission_info") 
sequencing_dict <- set_names(sequencing_table$Isolate, sequencing_table$Submission_id)
samples_table <- isolate_table %>% arrange(desc(Pathogenicity), desc(Collection_Year), Site)

#figs(name="WtFreq1","Weight frequency of Ruffe captured in 1992.")

```

# Experimental Design
In 2017, DNA was extracted from 21 strains of _Ascochyta rabiei_ and sent for Whole-Genome-Sequencing (WGS) on an Illumina HiSeq2500, producing 100 bp short paired-end reads (Macrogen, Korea).  
In the following year (2018), DNA from 20 additional *A. rabiei* isolates was extracted and sent for WGS, first to AgriBio, Centre for AgriBioscience, Agriculture Victoria Research on a HiSeq3000, producing 150 bp paired-end reads. Since the library preparation and sequencing was substantially delayed, 18 DNA samples, mostly overlapping with the 20 samples sent for AgriVic, were sent for sequencing at the Australian Genome Research Facility (AGRF, Melbourne) on 4 lanes of a NextSeq500 flowcell, producing 150 bp paried-end reads (run name CAGRF19461).  

## Methods
DNA-Seq data processing, mapping and variant calling were performed on the _Griffith University Gowonda HPC Cluster_ (using Torque scheduler), following the methods specified by @hagiwara_whole-genome_2014, @haas_approaches_2011, @hittalmani_novo_2016 and @verma_draft_2016, with modification to use `r fontFmt("FreeBayes")` v1.2.0 [@garrison_haplotype-based_2012] to assign variant probability scores and call variants.  
An alternative approach was tested, running the entire pipeline with tools from the `r fontFmt("BBtools")` (v38.22; @bushnell_bbmap:_2014) suite, as detailed in the [BBtools section](#bbtools).

#### Adaptor Trimming
Adaptors needed to be removed, as well as very low quality bases/reads, so trimming was performed with BBduk (from BBMap v38.34). See official download page on [SourceForge](https://sourceforge.net/projects/bbmap/), [user guide](http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/) and [SEQanswers thread](http://seqanswers.com/forums/showthread.php?t=42776).  

### Mapping to the reference genome (using Bowtie2)
The trimmed reads were mapped to the _A. rabiei_ reference genome, strain Me14 [@verma_draft_2016] using Bowtie2 (version 2.3.4.3) with the `--very-sensitive` option in `--end-to-end` (global) mode. The resulting SAM files were processed to mark duplicates with SAMBLASTER v0.1.24 [@faust_samblaster:_2014] and converted into coordinate-sorted, indexed BAM files with read groups using [Picard v2.18.22](https://broadinstitute.github.io/picard/).     
Raw files quality after adaptor trimming was assessed with `r fontFmt("FastQC")` (v0.11.8). Post-trimming and mapping statistics were consolidated into a single, interactive report for each batch using `r fontFmt("MultiQC")` v1.0 [@ewels_multiqc:_2016]

#### Bowtie2 mapping

```{bash trim_reads_macrogen}
cd ~/data/A_rabiei_sequencing/Macrogen_sequences_1702KHP-0164
# fix read file names
my_rename -v 's/_([12].fastq)/_R\1/' *.fastq.gz
# Remove duplicate files
my_rename -v 's/.gz/.gz.dup/' 15CUR005*.fastq.gz
# Prepare the commands
DATE=`date +%d_%m_%Y`
BATCH=macrogen
RUN="${BATCH}_BT2_process_${DATE}" # day of run was 02_02_2019
mkdir ${RUN}
cd !$
GENOME="$HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/A.rabiei_me14"
PLOIDY=1
NCORES=12
# These will change for each batch
READ_LENGTH=100
RGPL=HiSeq2500 
RGPU=HHCLJBCXY
RGCN=Macrogen 
# trim adapters, map, remove duplicates, sort, index and add Read Group information
bbduk.sh -Xmx1g ref=${BBMAP_DIR}/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 qtrim=rl trimq=10 tpe tbo int minlen=30 ziplevel=9 ow in=read_file_R#.fq.gz out=trimmed_read_file_R#.fastq.gz ow && bowtie2 --very-sensitive -p %s -x $GENOME.fasta -1 trimmed_read_file_R1.fastq.gz -2 trimmed_read_file_R2.fastq.gz --un-conc-gz read_file_unmapped.fq.gz --no-unal | samblaster | picard AddOrReplaceReadGroups I=/dev/stdin O=read_file_BT2_me14.dedup.rg.csorted.bam SM=read_file ID=read_file LB=read_file PL=$RGPL PU=$RGPU CN=$RGCN CREATE_INDEX=true  SO=coordinate 

```

>Note that for the AGRF batch, the read files from all lanes were combined together and then processed as above.


#### Variant calling using FreeBayes
The alignment files were merged once more into a single file that was used as input for `r fontFmt("FreeBayes")` v1.2.0 [@garrison_haplotype-based_2012] to assign variant probability scores and call variants.

```{bash FB_var_call_ind}
PLOIDY=1
# JOB_NAME="BBmap_me14_vars"
GENOME="$HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/A.rabiei_me14"
NCORES=12

# merge ind files to a single bam file (keep submission ids)
BAM_FILES=$( ls -1 *.csorted.bam | gawk -vORS=" " '1' )
sambamba merge -t $NCORES all_ind.csorted.combined.bam $BAM_FILES

# calculate coverage
sambamba depth base --combined all_ind.csorted.combined.bam | cut -f 1-3 | tail -n +2 | coverage_to_regions.py $GENOME.fasta.fai 500 > targets.regions 
# create parallel jobs to run (can be used with --dry-run flag to write the commands into a file that can be submitted to an HPC cluster)
cat targets.regions | parallel -k "freebayes -p $PLOIDY -f $GENOME.fasta all_ind.csorted.combined.bam --region {} > freebayes_target_regions.output{#}"
# gather data and remove duplicate headers
cat freebayes_target_regions.output* | vcffirstheader | vcfstreamsort -w 1000 | vcfuniq > A_rabiei_2018_isolates_haplo_ind.bt2.fb.vcf
```

The error rates can be estimated by counting the number of non-matching genotypes/loci across the entire VCF file based on duplicate sequenced samples from the same isolate.

#### VCF filtering
Variants were filtered using `r fontFmt("SnpSift")` v4.3.1 [@ruden_using_2012], based on their total loci depth, keeping ony SNP loci with at least 100 reads covering the locus (DP $\ge 100$) and not more than 20000 (based on EDA). In addition, each genotype call was removed (recoded as `./.`) if it had read depth <5.
```{bash vcf_filter}
# Recode genotypes as missing if below a certain threshold, such as genotyping quality or depth (GQ:DP)  
# filter only polymorphic SNPs, using QUAL>20, DP<36000/20000
QUAL=10
MAX_DP=20000
MIN_DP=20
# filter individual vcf
SnpSift filter "( QUAL>$QUAL ) & ( TYPE='snp' ) & ( DP<$MAX_DP ) & ( DP>$MIN_DP ) & ( countRef()>=1 & countVariant()>=1 )" A_rabiei_2018_isolates_haplo_ind.bt2.fb.vcf | SnpSift gtfilter -gv './.' "( DP<5 )" > A_rabiei_2018_isolates_haplo_ind.bt2.fb.Qual${QUAL}.U${DP}DP.poly.snps.vcf
```

### Mapping and variant calling usibg BBtools {#bbtools}

NextSeq quality scores are known to be inaccurate (due to binnig and using just 2 colour system, see [SEQanswers discussion](http://seqanswers.com/forums/showpost.php?p=172507&postcount=120)), so it's advised to recalibrate them before any quality trimming. The reads were therefore combined, trimmed to remove sequencing adaptors only (not based on quality) with `r fontFmt("bbduk.sh")` and then mapped to the reference genome (with `r fontFmt("bbmap.sh")`) to calculate quality recalibration matrices (with `r fontFmt("calctruequality.sh")`). Each read pair (R1 and R2) were then further filtered (with `r fontFmt("filterbytile.sh")`), deduped (with `r fontFmt("clumpify.sh")`), quality-recalibrated and trimmed (with `r fontFmt("bbduk.sh")`) and mapped to the _A. rabiei_ reference genome, strain _Me14_ (Rob Lee, Curtin University), using `r fontFmt("bbmap.sh")` with the `k=12` option in `slow` mode for high sensitivity, in preparation for variant calling (see this [discussion](http://seqanswers.com/forums/showpost.php?p=208146&postcount=233)). The entire pipeline was performed using the specified tools from `r fontFmt("BBtools")` (v38.22; @bushnell_bbmap:_2014). See official download page on [SourceForge](https://sourceforge.net/projects/bbmap/), [user guide](http://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/) and [SEQanswers thread](http://seqanswers.com/forums/showthread.php?t=42776).  
Raw files quality after adaptor trimming was assessed with `r fontFmt("FastQC")` (v0.11.8). The aligned SAM files were sorted and compressed to BAM format with [Picard v2.18.22](https://broadinstitute.github.io/picard/). Post-trimming and mapping statistics were consolidated into a single, interactive report for each batch using `r fontFmt("MultiQC")` v0.9.1 [@ewels_multiqc:_2016]

```{bash recal_trim_reads_macrogen}
GENOME="$HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/A.rabiei_me14"
PLOIDY=1
READ_LENGTH=100 # adjust per batch
RGPL=HiSeq2500 # adjust per batch
RGPU=HHCLJBCXY
RGCN=Macrogen
case $RGPL in
    NextSeq )
        CLUMP_PARAM="dupedist=40 spany" ;;
    HiSeq2500 )
        CLUMP_PARAM="dupedist=40" ;;
    HiSeq3000 )
        CLUMP_PARAM="dupedist=2500" ;;
    HiSeq4000 )
        CLUMP_PARAM="dupedist=2500" ;;
    Novaseq )
        CLUMP_PARAM="dupedist=12000" ;;
    * )
        CLUMP_PARAM="" ;;
esac
# Combine all _1 and _2 reads (ignoring contaminated/low quality samples)
ls -1 ./*_R1.fastq.gz | sort | xargs cat > raw_R1.fq.gz
ls -1 ./*_R2.fastq.gz | sort | xargs cat > raw_R2.fq.gz

# Create the index, trim adapters, mapp and calculate quality matrices
bbduk.sh in=raw_R#.fq.gz out=stdout.fq ref=$BBMAP_DIR/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 minlen=$READ_LENGTH tpe tbo int=f | bbmap.sh in=stdin.fq out=raw.sam ref=$GENOME.fasta ambig=toss slow int && calctruequality.sh in=raw.sam ref=$GENOME.fasta callvariants ploidy=$PLOIDY

# Then perform adapter trimming, mapping, clumping, recalibration
# make sure that CLUMP_PARAM are adjusted for each read set  
find ./ -maxdepth 1 -name "*_R1.fastq.gz" | sort | gawk -F"\t" -v bbmap_dir=$BBMAP_DIR -v genome=$GENOME -v RGPL=$RGPL -v RGPU=$RGPU -v RGCN=$RGCN -v CLUMP_PARAM=$CLUMP_PARAM  'BEGIN{command=sprintf("bbduk.sh -Xmx1g ref=%s/resources/adapters.fa ktrim=r k=23 mink=11 hdist=1 qtrim=rl trimq=10 tpe tbo int minlen=30 ziplevel=9 ow", bbmap_dir)};{n=split($1,a,"/"); infile=gensub("_R1\\.", "_R#.", "1", $1); basename=gensub(/(.+)_R1.fastq.gz/, "\\1", "1", a[n]) ; printf("cd $PBS_O_WORKDIR; filterbytile.sh in=%s out=stdout.fq int=f | clumpify.sh in=stdin.fq out=stdout.fq int dedupe optical %s adjacent | bbduk.sh in=stdin.fq out=stdout.fq int recalibrate |  %s in=stdin.fq out=recal_trimmed_%s_R#.fastq.gz stats=%s.stats ow && bbmap.sh in=recal_trimmed_%s_R#.fastq.gz outm=stdout.sam outu=%s_bbmap_me14.unmapped.fq.gz machineout local ow int=f rgsm=%s rgid=%s_%s rgpl=%s rgpu=%s rgcn=%s pigz unpigz slow k=12 | picard SortSam I=/dev/stdin O=%s_bbmap_me14.csorted.bam SO=coordinate\n",infile, CLUMP_PARAM, command, basename, basename, basename, basename, basename, basename, NR, RGPL,RGPU, RGCN, basename)}' > ${RUN}.bash

# run the commands using GNU-parallel or submit as a job aray to an HPC
```

> Note that for each batch, different READ_LENGTH and CLUMP_PARAM were used (files starting with AGRF are NextSeq500, 150PE; files F16253-1 came from HiSeq2500, 100PE and files 7_S40 were produced by HiSeq3000, 150PE), so please adjust the parameters accordingly for each file set.

#### Call Variants using BBtools
Variants were called from the recalibrated alignment files using `r fontFmt("callvariants.sh")`, adjusted to call rare variants (MAF=0.02) and ignore the first and last 20 bp at the ends of the reads (as discussed in [SeqAnswers](http://seqanswers.com/forums/showpost.php?p=208194&postcount=235)).
One process called variants for duplicate samples, to identify error rates that occur from the sequencing and different platforms. For the final VCF file, the sam files of duplicate samples were combined to increase coverage and standardise sample names (as isolates) and another `r fontFmt("callvariants.sh")` was performed.

```{bash call_dup_sams}
GENOME="$HOME/scratch/data/reference_genomes/Ascochyta_reference_genomes/A.rabiei_me14"
PLOIDY=1
# call duplicate sams
# save table of duplicated samples
DUP_BAMS=$( ls -1 *.csorted.bam | gawk -vORS="," '1' )
DUP_SAMPLES=$( echo $DUP_BAMS | sed 's/_bbmap_me14.csorted.bam//g; s/ /,/g' )

# Call variants (needs lots of memory, but very quick)
callvariants.sh in=$DUP_BAMS sample=$DUP_SAMPLES maf=0.02 rarity=0.02 minreads=5 multisample ploidy=$PLOIDY border=20 out=A_rabiei_2018_dup_samples.var vcf=A_rabiei_2018_dup_samples.vcf ref=$GENOME.fasta

```

### Compare error rates

```{r error_rates, eval=FALSE}
# load sequencing table
sequencing_table <- readxl::read_excel("./sample_info/A_rabiei_isolate_list_for_wgs.xlsx",
                                       sheet = "submission_info") %>% #arrange(Isolate) %>%
  group_by(Isolate) %>%
  mutate(counter=row_number(), Isolate_head=paste(Isolate, counter, sep="."))

samples_dict <- setNames(sequencing_table$Isolate_head, sequencing_table$Submission_id)
source("./src/estimate_error_rates_vcf_files.R")
# load VCF file (Bowtie2 pipeline)
filtered_vcf_files <- c("../FB_vars_BT2_07_04_2019/A_rabiei_2018_isolates_haplo_ind.bt2.fb.Qual10.U20000DP.poly.snps.vcf", "../FB_vars_BBtools_01_03_2019/A_rabiei_2018_isolates_haplo_ind.bbtools.fb.Qual10.U20000DP.poly.snps.vcf")
# error_rates <- tibble()
for (f in filtered_vcf_files){
  # check error rates between replicate samples

  filtered_vcf <- read.vcfR(f)
  vcf_headers <- colnames(filtered_vcf@gt)[-1]
  sample_isolates <- samples_dict[vcf_headers]
  colnames(filtered_vcf@gt)[-1] <- sample_isolates
  fixed_vcf_file <- paste0(f, ".fixed.gz")
  write.vcf(x=filtered_vcf,file = fixed_vcf_file)
  R.utils::gunzip(fixed_vcf_file, overwrite=TRUE)

  error_rates <- rbind(error_rates, estimate_error_rates(paste0(f, ".fixed"), grouping_suffix = ".[0-9]+$"))
}
# Check how to compare the error rates for each group.
error_rates_sum <- error_rates %>% arrange(replicate_group) %>%
  mutate(analysis=sub("FB_vars_", "", str_extract(vcf_file, "FB_vars.+_2019"))) %>%
  # mutate(analysis=sub("01_03", "BBtools_01_03", sub("07_04", "BT2_07_04", analysis))) %>%
  dplyr::select(analysis, replicate_group, allele_error_rate) %>%
  # group_by(replicate_group) %>%
  spread(key=analysis, value = allele_error_rate) %>%
  mutate(allele_error_rate_diff=BBtools_01_03_2019-BT2_07_04_2019)
sprintf("%.2f%%", mean(error_rates_sum$BBtools_01_03_2019)*100)
```

```{r error_table, eval=TRUE}
error_rates_sum <- readxl::read_excel("output/results/pipeline_comparison.xlsx")
datatable(as.data.frame(error_rates_sum), caption=tbls("error_rates"), 
          rownames = FALSE)  %>% 
  formatStyle('BBtools_01_03_2019',
  target = 'row',
  backgroundColor = styleInterval(0.09, c(NA, 'yellow'))) %>% 
  formatPercentage(2:4, 2)


```


### Conclusions
The `vcf` file from the BBtools pipeline came out with only the reference alleles at all loci, across all samples.  
Following that, the BBtools recalibrated alignment files were as input to FreeBayes and to compare to the ones produced by Bowtie2. When comparing genotypes calls between samples at each loci to estimate error rates, the variant file produced from the recalibrated alignments (BBmap) showed an average allele error rates of `r sprintf("%.2f%%", mean(error_rates_sum$BBtools_01_03_2019)*100)`, compared with `r sprintf("%.2f%%", mean(error_rates_sum$BT2_07_04_2019)*100)` resulting from the Bowtie2 alignments. In some cases, the error rates from the BBmap-derived variants was close to 10% (see highlighted row in `r caption=tbls("error_rates", display="cite")`)


***
## Bibliography

<!-- ```{r results='asis', eval=TRUE} -->
<!-- PrintBibliography(biblio) -->
<!-- ``` -->

